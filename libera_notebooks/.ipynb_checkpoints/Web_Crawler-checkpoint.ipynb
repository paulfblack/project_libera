{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import SoupStrainer\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import smtplib\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from urllib.parse import urlparse\n",
    "from pymongo import MongoClient\n",
    "import pymongo\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "utility_password = os.environ['email_bot_password']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "password = os.environ['password']\n",
    "libera_url = \"54.164.158.211\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E-mail notice functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def requests_error_notice(error_code,current_url):\n",
    "    \"\"\"Sends an e-mail notification if requests fails to connect to a url\"\"\"\n",
    "    smtpObj = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "    smtpObj.ehlo()\n",
    "    smtpObj.starttls()\n",
    "    smtpObj.login('pfblack.utility@gmail.com',  utility_password)\n",
    "    smtpObj.sendmail('pfblack.utility@gmail.com', 'paul.laifu.black@gmail.com',\n",
    "                     'Subject: Get request failed\\nRequests failed to scrape the following url\\n \\\n",
    "                     url: %s \\n \\\n",
    "                     error code: %i (if 0 requests failed to connect)' % (current_url, error_code))\n",
    "    smtpObj.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def duplicate_notice(db_type, db, col, url, duplicate_count):\n",
    "    \"\"\"Sends an e-mail notification if a url has been found in the database multiple times\"\"\"\n",
    "    smtpObj = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "    smtpObj.ehlo()\n",
    "    smtpObj.starttls()\n",
    "    smtpObj.login('pfblack.utility@gmail.com', utility_password)\n",
    "    smtpObj.sendmail('pfblack.utility@gmail.com', 'paul.laifu.black@gmail.com',\n",
    "                     'Subject: Duplicate Found\\nA duplicate entry has been found on your %s database.\\n \\\n",
    "                     Database: %s \\n \\\n",
    "                     Collection: %s \\n \\\n",
    "                     Url: %s \\n \\\n",
    "                     Count: %i' % (db_type, db, col, url, duplicate_count))\n",
    "    smtpObj.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def termination_notice(location):\n",
    "    \"\"\"Sends an e-mail notification if function/process terminates/concludes\n",
    "    \n",
    "    Note: Requires that no unhandled errors were thrown and that the function concluded normally\n",
    "    \"\"\"\n",
    "    smtpObj = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "    smtpObj.ehlo()\n",
    "    smtpObj.starttls()\n",
    "    smtpObj.login('pfblack.utility@gmail.com', utility_password)\n",
    "    smtpObj.sendmail('pfblack.utility@gmail.com', 'paul.laifu.black@gmail.com',\n",
    "                     'Subject: Progam Stopped Running\\nYour WebCrawler has terminated on %s!' % location)\n",
    "    smtpObj.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "client = pymongo.MongoClient(\"mongodb://paul:\" + password + \"@\" + libera_url + \"/libera_db\")\n",
    "\n",
    "db = client.libera_db\n",
    "\n",
    "col = db.scraped_blogs\n",
    "\n",
    "only_body_tag = SoupStrainer(\"body\")\n",
    "\n",
    "http_pattern = re.compile('https://www.|https://|http://www.|http://|www.')\n",
    "\n",
    "location = 'local computer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pickle.load(open(\"mongodb_data.p\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_crawler(max_iterations, location):\n",
    "    for _ in range(max_iterations):\n",
    "        current_url = get_seed()\n",
    "        source_code = make_request(current_url)\n",
    "        if source_code:\n",
    "            soup = BeautifulSoup(source_code.text, \"lxml\", parse_only=only_body_tag)\n",
    "            get_links(soup, current_url)\n",
    "        else:\n",
    "            pass\n",
    "    termination_notice(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_seed():\n",
    "    \"\"\"Grabs an open seed from the libera_db collection: scraped blogs\n",
    "    and then sets the value of the open_seed field to False.\n",
    "    \n",
    "    $natural, 1 sets the order of scan to create order\n",
    "    \"\"\"\n",
    "    current_url = col.find({'open_seed':True}).hint([('$natural', 1)]).limit(3)[2]['url']\n",
    "    col.update_one({'url':current_url},{'$set':{'open_seed':False}})\n",
    "    return current_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Used to pull up soup's plain text from seed AND to iterate through link_list\n",
    "def make_request(current_url):\n",
    "    \"\"\"Takes a url and returns the BeautifulSoup object of the webpage\"\"\"\n",
    "    try:\n",
    "        source_code = requests.get(current_url)\n",
    "        time.sleep(1)\n",
    "        # If requests connects, but does not succeed in getting source_code, return None, e-mail error_notice, edit data entry\n",
    "        if source_code.status_code != 200:\n",
    "            error_code = source_code.status_code\n",
    "            col.update_one({'url':current_url},{'$set':{'failed_scrape':True, 'error_code':source_code.status_code}})\n",
    "            requests_error_notice(error_code, current_url)\n",
    "            source_code = None\n",
    "    # if request fails to connect, return None, e-mail error_notice, edit database entry\n",
    "    except:\n",
    "        error_code = 0\n",
    "        requests_error_notice(error_code, current_url)\n",
    "        col.update_one({'url':current_url},{'$set':{'failed_scrape':True, 'error_code':error_code}})\n",
    "        source_code = None\n",
    "    return source_code\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check link before expanding to save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Be sure to define current url\n",
    "def get_links(soup, current_url):\n",
    "    \"\"\"Grabs all links from current_url's soup.\n",
    "\n",
    "    Will pass on invalid links, links that match the avoid pattern, \n",
    "    and links that have already been grabbed.\n",
    "\n",
    "    New Links will have open_seed: True meaning they can be used as a seed for a later step\n",
    "    This value will be set to False when this page is used as a seed\n",
    "    \"\"\"\n",
    "    # avoid pictures, files, and facebook pages,etc.\n",
    "    avoid_pattern = re.compile('zillow\\.com|coursera\\.org|yelp\\.com|flickr|tumblr|comments$|amazon.com|plus.google.com|linkedin.com|youtube.com|jobs.acm.org|vimeo.com|http://awards.acm.org|instagram.com|twitter.com|respond$|comment$|.pdf$|.png$|.jpg$|.jpeg$|.gif$|.xlsx$|wikipedia.org|facebook|\\?share=|\\?ref=footer_website|\\?ref=footer_blog')\n",
    "    for a_tag in soup.find_all('a'):\n",
    "        href =a_tag.get('href')\n",
    "        # Make sure that link is valid and not in avoid patterns, otherwise pass\n",
    "        if href != None and http_pattern.match(href) and not avoid_pattern.search(href):\n",
    "            # Check to see if the link has already been grabbed.\n",
    "            if previously_grabbed(href):\n",
    "                # if it has been previously grabbed AND it's an external link inc inbound_link_count and urls_pointed_here\n",
    "                if external_link(href, current_url):\n",
    "                    col.update_one({'url':href}, {'$inc': {'inbound_link_count':1}, '$push': {'urls_pointed_here':current_url}})\n",
    "                pass\n",
    "            # If not previously grabbed, expand link and check again\n",
    "            else:\n",
    "                source_code = make_request(href)\n",
    "                # Try to connect via requests lib and if failed pass\n",
    "                if source_code:\n",
    "                    # failsafe for situations where url has been shortened, expands url\n",
    "                    try:\n",
    "                        url = source_code.url\n",
    "                    except AttributeError:\n",
    "                        requests_error_notice(0, href)\n",
    "                        continue\n",
    "                    # Query SQL or MongoDB to see if link has already been grabbed\n",
    "                    if previously_grabbed(url):\n",
    "                        # If the link points to a new web page increase that url's inbound_link_count by one\n",
    "                        if external_link(url, current_url):\n",
    "                            col.update_one({'url':url}, {'$inc': {'inbound_link_count':1}, '$push': {'urls_pointed_here':current_url}})\n",
    "                        pass\n",
    "                    # Only add a url to open_seed list if it has not been previously grabbed and it is valid\n",
    "                    # If it is an external link, put inbound_link_count at 1    \n",
    "                    else:\n",
    "                        try:\n",
    "                            blog_text = scrape_url(source_code)\n",
    "                            update_blog_entry(url, blog_text, current_url)\n",
    "                        except AttributeError:\n",
    "                            requests_error_notice(0, href)\n",
    "                            pass\n",
    "                else:\n",
    "                    pass\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function for get_links\n",
    "def external_link(url, current_url):\n",
    "    \"\"\"Checks to see if a link is internal or external.\n",
    "    This is for initializing a mock page-rank system:\n",
    "        Mock page-rank system will tally up the number\n",
    "        of incomming links from external web pages\n",
    "        across scraped pages.\"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    parsed_current_url = urlparse(current_url)\n",
    "    # if both the scheme and the netloc match between sites it is an internal link, return false\n",
    "    if parsed_url.scheme == parsed_current_url.scheme and parsed_url.netloc == parsed_current_url.netloc:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper function for get_links\n",
    "def previously_grabbed(url):\n",
    "    \"\"\"Check to see if url is already in DataBase\n",
    "    If it has been previously scraped it will return True and webcrawler will act accordingly\n",
    "    If it exists more than once, something has gone wrong and an e-mail will be sent out to notify users\n",
    "    If it has not yet been scrapped it returns false\n",
    "    \"\"\"\n",
    "    if col.find({'url':url}).count() == 1:\n",
    "        return True\n",
    "    \n",
    "    # if the url exists multiple times e-mail out notification, something went wrong!\n",
    "    elif col.find({'url':url}).count() > 1:\n",
    "        duplicate_count = col.find({'url':url}).count()\n",
    "        duplicate_notice(db_type, db, col, url, duplicate_count)\n",
    "        return True\n",
    "    \n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_url(source_code):\n",
    "    \"\"\"Takes a live url and it's BeautifulSoup object and pulls all the text from the body\n",
    "    Does not pull from inputs, scripts, and noscripts (to avoid JavaScript)\n",
    "    Returns the text as blog_text\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(source_code.text, \"lxml\", parse_only=only_body_tag)\n",
    "    blog_text = []\n",
    "    # still grabbing some javascript\n",
    "    for child in soup.body.children:\n",
    "        if child.name != None and child.name != 'script' and child.name != 'input' and child.name != 'noscript' and child.name != 'style' and child.name!= 'option':\n",
    "            blog_text.append(child.get_text(' ', strip=True))\n",
    "    space = ' '\n",
    "    blog_text = space.join(blog_text)\n",
    "    blog_text.encode('utf-8')\n",
    "    \n",
    "    return blog_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Needs to be Reworked! But for now we're turning it off to accept all pages\n",
    "# Runs inside of update_blog_entry\n",
    "def quality_check(url, blog_text):\n",
    "    \"\"\"Takes a url the scraped_text and passes it through a quality check.\n",
    "    \n",
    "    For this project that quality check is a Naive Bayes Classifier trained on categorized\n",
    "    blog posts from initial scraping.\n",
    "    \n",
    "    If the blog_text meets relevancy criteria, return True, else return False\n",
    "    \"\"\"\n",
    "    # Insert Naive Bayes classification here, if pass return True if fail return False\n",
    "    \"\"\"Example psuedo-code:\n",
    "    category = nb_classifier.predict(blog_text)\n",
    "    if category == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\"\"\"\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_blog_entry(url, blog_text, current_url):\n",
    "    \"\"\"If blog_text passes the quality check, update it's entry in mongo_db\n",
    "    Otherwise set open_seed to False and drop blog_text information\n",
    "    \n",
    "    For relevant web pages:\n",
    "    \n",
    "    hand_reviewed:\n",
    "    --- True if this entry has been hand reviewed\n",
    "    --- False if this entry has not been hand reviewed\n",
    "    \n",
    "    quality:\n",
    "    --- True if this web page passed quality check\n",
    "    --- False if this web page failed quality check\n",
    "    \"\"\"\n",
    "    if quality_check(url, blog_text):\n",
    "        if external_link(url, current_url):\n",
    "            col.insert_one(\n",
    "                {'url': url,\n",
    "                 'text': blog_text,\n",
    "                 'hand_reviewed': False,\n",
    "                 'quality': True,\n",
    "                 'open_seed':True,\n",
    "                 'inbound_link_count':1,\n",
    "                 'urls_pointed_here':[current_url]\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            col.insert_one(\n",
    "                {'url': url,\n",
    "                 'text': blog_text,\n",
    "                 'hand_reviewed': False,\n",
    "                 'quality': True,\n",
    "                 'open_seed':True,\n",
    "                 'inbound_link_count':0\n",
    "                }\n",
    "            )\n",
    "    else:\n",
    "        col.insert_one(\n",
    "            {'url':url,\n",
    "            'hand_reviewed':False,\n",
    "            'quality': False\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_crawler(max_iterations, location):\n",
    "    for _ in range(max_iterations):\n",
    "        current_url = get_seed()\n",
    "        source_code = make_request(current_url)\n",
    "        if source_code:\n",
    "            soup = BeautifulSoup(source_code.text, \"lxml\", parse_only=only_body_tag)\n",
    "            get_links(soup, current_url)\n",
    "        else:\n",
    "            pass\n",
    "    termination_notice(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_iterations = 225\n",
    "run_crawler(max_iterations, 'aws')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
