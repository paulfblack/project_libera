{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GAME PLAN\n",
    "If it fails to scrape add those links to a reseed list to try again\n",
    "    -> get more explicit in exception catching/case handling\n",
    "Only go one click deep \n",
    "Set up Flask based website dating app\n",
    "Hit Links to SQL\n",
    "New Text to Mongo\n",
    "Be better at scraping new text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "Need to update mongodb 'text' entries to use consistent webscraping mechanism, then check the follow url for javascript: 'http://www.datasciencecentral.com/profiles/blogs/how-you-can-improve-customer-experience-with-fast-data-analytics'  \n",
    "  \n",
    "This url uses xml and will be a good test point to see if JS is still coming through.  \n",
    "\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Need to handle pymongo timeout errors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scrape as you go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import SoupStrainer\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import smtplib\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from urllib.parse import urlparse\n",
    "from pymongo import MongoClient\n",
    "import pymongo\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "password = os.environ['password']\n",
    "libera_url = \"54.164.158.211\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = col.aggregate( [ {'$match': {'quality':True}},{'$sample' : {'size': 2}}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = y.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = y.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('593f8ed9f7e49c258ed381bd'),\n",
       " 'hand_reviewed': False,\n",
       " 'inbound_link_count': 0,\n",
       " 'open_seed': True,\n",
       " 'previous_quality': True,\n",
       " 'quality': True,\n",
       " 'text': \"// <![CDATA[\\n        var disqus_shortname = 'bigdatamadesimple';\\n        (function () {\\n            var nodes = document.getElementsByTagName('span');\\n            for (var i = 0, url; i < nodes.length; i++) {\\n                if (nodes[i].className.indexOf('dsq-postid') != -1) {\\n                    nodes[i].parentNode.setAttribute('data-disqus-identifier', nodes[i].getAttribute('rel'));\\n                    url = nodes[i].parentNode.href.split('#', 1);\\n                    if (url.length == 1) { url = url[0]; }\\n                    else { url = url[1]; }\\n                    nodes[i].parentNode.href = url + '#disqus_thread';\\n                }\\n            }\\n            var s = document.createElement('script'); s.async = true;\\n            s.type = 'text/javascript';\\n            s.src = '//' + 'disqus.com/forums/' + disqus_shortname + '/count.js';\\n            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);\\n        }());\\n    //]]> Sectors Banking / Finance Retail / eCom Travel / Hospitality Privacy / Security Marketing Telecommunication Media / FMCG Crime / Law Sports Health / Pharma Education Human Resource Tech & Tools Analytics Data Science Business Intelligence Digital Personalization Machine Learning Artificial Intelligence Visualisation Hadoop Data Mining SQL NoSQL Events Resources Video Books e-Books My Favourites Write For Us  Big Data - Made Simple Login Resources Resources Resources Video Books e-Books Analytics 3 effective methods of data collection for market research 06th Jun `17 11:55 AM Comments Share Favorite Analytics Look at what you can do with these 4 excel reporting tools 05th Jun `17 11:35 AM Comments Share Favorite Books The Manual for Indian Startups In conversation with lead author, Vijaya Kumar Ivaturi 31st May `17 13:20 PM Comments Share Favorite Resources How bad data changed the course of history [Infographic] 04th May `17 13:28 PM Comments Share Favorite Resources Nine ways today’s entrepreneurs use big data to improve their businesses 09th Apr `17 12:21 PM Comments Share Favorite Resources 3.14159 ways to celebrate Pi Day 17th Mar `17 11:42 AM Comments Share Favorite Resources 4 reasons startups must unlock the potential of big data 04th Feb `16 11:21 AM Comments Share Favorite Data Science 30 tweetable quotes about Data Science 17th Sep `15 10:58 AM Comments Share Favorite Resources 10 Indian Data Scientists you should know 08th Sep `15 10:43 AM Comments Share Favorite Resources What big data doesn’t show us: Experts share their views 02nd Sep `15 11:21 AM Comments Share Favorite Resources How to use the information collected for your business 29th Aug `15 21:28 PM Comments Share Favorite Analytics It’s hard to be a data-driven organization 27th Aug `15 14:06 PM Comments Share Favorite MORE STORIES Have a story to share? There are many ways you can contribute to help people understand big data. Write Here MORE FROM BIG DATA MADE SIMPLE Precision marketing uses big data, mobility for insight on consumers The use of precision marketing, which uses location-based mobile services and big data analytics to gather insight on… In Marketing Case study: How big data powers the eBay customer journey With 50TB of machine-generated data produced daily and the need to process 100PB of data all together, eBay’s… In Retail / eCom 5 best countries to host your website for data privacy Here are some of the countries with the strongest data privacy laws for websites and the press. 1…. In Privacy / Security Fourth industrial revolution brings challenges to Big Data Enabled by communications and sensor technology, the global manufacturing business is on verge of what some are calling… In Resources How I chose the right programming language for Data Science Which programming language should I learn for getting started in data science? That’s one of the most popular… In Data Science , by Manu Jeevan on Sep 04 Biggest data security breaches since 2010 All businesses and organizations have legal and ethical responsibilities to safeguard their confidential data as well as their… In Privacy / Security , by Rajkumar P on Aug 12 5 steps to get actionable insights from raw data Companies are heavily investing in acquiring and developing talent, technology and business processes aimed at collecting and analyzing… In Analytics Should you invest in a BI tool in the first 2 years of your company The tech and business worlds are raving about the expansion of the Internet of Things (IoT), and the… In Business Intelligence , by Christopher Burge on Sep 19 Eye-opening facts everyone should know about Big Data This article was first published on Linkedin by\\xa0best-selling author and keynote speaker Bernard Marr. We surely see a… In Resources , by Bernard Marr on Oct 14 How to teach an artificial brain to understand right and wrong Movies like I, Robot, Wall-e and Bicentennial Man pose interesting questions that blur lines between man and machines…. In Artificial Intelligence , by Vivek Debuka on Feb 23 10 R packages for Machine Learning Needless to say, R is one of the most efficient and effective tools for analysing and manipulating data… In Machine Learning , by Payel Roychoudhury on Jun 22 Silicon Valley legend and former Intel CEO Andy Grove Passes away at 79 Intel has announced that the company’s former CEO and chairman, Andrew S. Grove, who was born in Hungary… In General OUR PARTNERS About us Newsletter Archives RSS feed Contributors Contact Us Subscribe to our Newsletter Subscribe Powered by Copyright © 2017 Crayon Data . All rights reserved.\",\n",
       " 'url': 'http://bigdata-madesimple.com/category/resources/'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for x in y:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://101.datascience.community/tag/education/'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('59486713f7e49c0a9f7ba471'), 'url': 'http://abbottanalytics.blogspot.com/search/label/art', 'hand_reviewed': False, 'quality': False, 'open_seed': True, 'previous_quality': True}\n"
     ]
    }
   ],
   "source": [
    "for x in y:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class WebCrawler:\n",
    "#     def __init__(self):\n",
    "        \n",
    "# #         self.mongo_password = os.environ['password']\n",
    "# #         self.client = client = pymongo.MongoClient(\"mongodb://paul:\" + password + \"@\" + ip_address + \"/libera_db\")\n",
    "# #         self.db = self.client.libera_db\n",
    "# #         self.col = self.db.scraped_\n",
    "#     def get_soup(self, url):\n",
    "#         \"\"\"Takes a url and returns the BeautifulSoup object of the webpage\"\"\"\n",
    "#         source_code = requests.get(url)\n",
    "#         plain_text = source_code.text.encode('utf-8')\n",
    "#         soup = BeautifulSoup(plain_text, \"html.parser\")\n",
    "#         return soup\n",
    "#         time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Don't forget to start up mongodb on AWS instance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E-mail notice functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def requests_error_notice(error_code,current_url):\n",
    "    \"\"\"Sends an e-mail notification if requests fails to connect to a url\"\"\"\n",
    "    smtpObj = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "    smtpObj.ehlo()\n",
    "    smtpObj.starttls()\n",
    "    smtpObj.login('pfblack.utility@gmail.com', 'Metis1033')\n",
    "    smtpObj.sendmail('pfblack.utility@gmail.com', 'paul.laifu.black@gmail.com',\n",
    "                     'Subject: Get request failed\\nRequests failed to scrape the following url\\n \\\n",
    "                     url: %s \\n \\\n",
    "                     error code: %i (if 0 requests failed to connect)' % (current_url, error_code))\n",
    "    smtpObj.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def duplicate_notice(db_type, db, col, url, duplicate_count):\n",
    "    \"\"\"Sends an e-mail notification if a url has been found in the database multiple times\"\"\"\n",
    "    smtpObj = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "    smtpObj.ehlo()\n",
    "    smtpObj.starttls()\n",
    "    smtpObj.login('pfblack.utility@gmail.com', 'Metis1033')\n",
    "    smtpObj.sendmail('pfblack.utility@gmail.com', 'paul.laifu.black@gmail.com',\n",
    "                     'Subject: Duplicate Found\\nA duplicate entry has been found on your %s database.\\n \\\n",
    "                     Database: %s \\n \\\n",
    "                     Collection: %s \\n \\\n",
    "                     Url: %s \\n \\\n",
    "                     Count: %i' % (db_type, db, col, url, duplicate_count))\n",
    "    smtpObj.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def termination_notice(location):\n",
    "    \"\"\"Sends an e-mail notification if function/process terminates/concludes\n",
    "    \n",
    "    Note: Requires that no unhandled errors were thrown and that the function concluded normally\n",
    "    \"\"\"\n",
    "    smtpObj = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "    smtpObj.ehlo()\n",
    "    smtpObj.starttls()\n",
    "    smtpObj.login('pfblack.utility@gmail.com', 'Metis1033')\n",
    "    smtpObj.sendmail('pfblack.utility@gmail.com', 'paul.laifu.black@gmail.com',\n",
    "                     'Subject: Progam Stopped Running\\nYour WebCrawler has terminated on %s!' % location)\n",
    "    smtpObj.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "client = pymongo.MongoClient(\"mongodb://paul:\" + password + \"@\" + libera_url + \"/libera_db\")\n",
    "\n",
    "db = client.libera_db\n",
    "\n",
    "col = db.scraped_blogs\n",
    "\n",
    "only_body_tag = SoupStrainer(\"body\")\n",
    "\n",
    "http_pattern = re.compile('https://www.|https://|http://www.|http://|www.')\n",
    "\n",
    "location = 'local computer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pickle.load(open(\"mongodb_data.p\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "urls = df['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.bschool.com/blog/2011/40-fascinating-blogs-for-the-ultimate-statistics-geek/'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3813"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "value = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x7fd480cc7c18>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col.update_one({'url':'http://www.bschool.com/blog/2011/40-fascinating-blogs-for-the-ultimate-statistics-geek/'},{'$set':{'quality':value}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for x in range(len(urls)):\n",
    "    url = urls[x]\n",
    "    value = bool(quality[x])\n",
    "    col.update_one({'url':url},{'$set':{'quality':value}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3813"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quality = df['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('593f026ff7e49c1cd22aea40'),\n",
       " 'hand_reviewed': False,\n",
       " 'inbound_link_count': 1,\n",
       " 'open_seed': False,\n",
       " 'previous_quality': True,\n",
       " 'quality': True,\n",
       " 'text': '',\n",
       " 'url': 'http://olomon.com/yuebofa.php',\n",
       " 'urls_pointed_here': ['http://www.olomon.com/404.html']}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col.find({})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iterations = 1\n",
    "def run_crawler(max_iterations, location):\n",
    "    for _ in range(max_iterations):\n",
    "        current_url = get_seed()\n",
    "        source_code = make_request(current_url)\n",
    "        if source_code:\n",
    "            soup = BeautifulSoup(source_code.text, \"lxml\", parse_only=only_body_tag)\n",
    "            get_links(soup, current_url)\n",
    "        else:\n",
    "            pass\n",
    "    termination_notice(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_seed():\n",
    "    \"\"\"Grabs an open seed from the libera_db collection: scraped blogs\n",
    "    and then sets the value of the open_seed field to False.\n",
    "    \n",
    "    $natural, 1 sets the order of scan to create order\n",
    "    \"\"\"\n",
    "    current_url = col.find({'open_seed':True}).hint([('$natural', 1)]).limit(3)[2]['url']\n",
    "    col.update_one({'url':current_url},{'$set':{'open_seed':False}})\n",
    "    return current_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Used to pull up soup's plain text from seed AND to iterate through link_list\n",
    "def make_request(current_url):\n",
    "    \"\"\"Takes a url and returns the BeautifulSoup object of the webpage\"\"\"\n",
    "    try:\n",
    "        source_code = requests.get(current_url)\n",
    "        time.sleep(1)\n",
    "        # If requests connects, but does not succeed in getting source_code, return None, e-mail error_notice, edit data entry\n",
    "        if source_code.status_code != 200:\n",
    "            error_code = source_code.status_code\n",
    "            col.update_one({'url':current_url},{'$set':{'failed_scrape':True, 'error_code':source_code.status_code}})\n",
    "            requests_error_notice(error_code, current_url)\n",
    "            source_code = None\n",
    "    # if request fails to connect, return None, e-mail error_notice, edit database entry\n",
    "    except:\n",
    "        error_code = 0\n",
    "        requests_error_notice(error_code, current_url)\n",
    "        col.update_one({'url':current_url},{'$set':{'failed_scrape':True, 'error_code':error_code}})\n",
    "        source_code = None\n",
    "    return source_code\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check link before expanding to save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Be sure to define current url\n",
    "def get_links(soup, current_url):\n",
    "    \"\"\"Grabs all links from current_url's soup.\n",
    "\n",
    "    Will pass on invalid links, links that match the avoid pattern, \n",
    "    and links that have already been grabbed.\n",
    "\n",
    "    New Links will have open_seed: True meaning they can be used as a seed for a later step\n",
    "    This value will be set to False when this page is used as a seed\n",
    "    \"\"\"\n",
    "    # avoid pictures, files, and facebook pages,etc.\n",
    "    avoid_pattern = re.compile('zillow\\.com|coursera\\.org|yelp\\.com|flickr|tumblr|comments$|amazon.com|plus.google.com|linkedin.com|youtube.com|jobs.acm.org|vimeo.com|http://awards.acm.org|instagram.com|twitter.com|respond$|comment$|.pdf$|.png$|.jpg$|.jpeg$|.gif$|.xlsx$|wikipedia.org|facebook|\\?share=|\\?ref=footer_website|\\?ref=footer_blog')\n",
    "    for a_tag in soup.find_all('a'):\n",
    "        href =a_tag.get('href')\n",
    "        # Make sure that link is valid and not in avoid patterns, otherwise pass\n",
    "        if href != None and http_pattern.match(href) and not avoid_pattern.search(href):\n",
    "            # Check to see if the link has already been grabbed.\n",
    "            if previously_grabbed(href):\n",
    "                # if it has been previously grabbed AND it's an external link inc inbound_link_count and urls_pointed_here\n",
    "                if external_link(href, current_url):\n",
    "                    col.update_one({'url':href}, {'$inc': {'inbound_link_count':1}, '$push': {'urls_pointed_here':current_url}})\n",
    "                pass\n",
    "            # If not previously grabbed, expand link and check again\n",
    "            else:\n",
    "                source_code = make_request(href)\n",
    "                # Try to connect via requests lib and if failed pass\n",
    "                if source_code:\n",
    "                    # failsafe for situations where url has been shortened, expands url\n",
    "                    try:\n",
    "                        url = source_code.url\n",
    "                    except AttributeError:\n",
    "                        requests_error_notice(0, href)\n",
    "                        continue\n",
    "                    # Query SQL or MongoDB to see if link has already been grabbed\n",
    "                    if previously_grabbed(url):\n",
    "                        # If the link points to a new web page increase that url's inbound_link_count by one\n",
    "                        if external_link(url, current_url):\n",
    "                            col.update_one({'url':url}, {'$inc': {'inbound_link_count':1}, '$push': {'urls_pointed_here':current_url}})\n",
    "                        pass\n",
    "                    # Only add a url to open_seed list if it has not been previously grabbed and it is valid\n",
    "                    # If it is an external link, put inbound_link_count at 1    \n",
    "                    else:\n",
    "                        try:\n",
    "                            blog_text = scrape_url(source_code)\n",
    "                            update_blog_entry(url, blog_text, current_url)\n",
    "                        except AttributeError:\n",
    "                            requests_error_notice(0, href)\n",
    "                            pass\n",
    "                else:\n",
    "                    pass\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function for get_links\n",
    "def external_link(url, current_url):\n",
    "    \"\"\"Checks to see if a link is internal or external.\n",
    "    This is for initializing a mock page-rank system:\n",
    "        Mock page-rank system will tally up the number\n",
    "        of incomming links from external web pages\n",
    "        across scraped pages.\"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    parsed_current_url = urlparse(current_url)\n",
    "    # if both the scheme and the netloc match between sites it is an internal link, return false\n",
    "    if parsed_url.scheme == parsed_current_url.scheme and parsed_url.netloc == parsed_current_url.netloc:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper function for get_links\n",
    "def previously_grabbed(url):\n",
    "    \"\"\"Check to see if url is already in DataBase\n",
    "    If it has been previously scraped it will return True and webcrawler will act accordingly\n",
    "    If it exists more than once, something has gone wrong and an e-mail will be sent out to notify users\n",
    "    If it has not yet been scrapped it returns false\n",
    "    \"\"\"\n",
    "    if col.find({'url':url}).count() == 1:\n",
    "        return True\n",
    "    \n",
    "    # if the url exists multiple times e-mail out notification, something went wrong!\n",
    "    elif col.find({'url':url}).count() > 1:\n",
    "        duplicate_count = col.find({'url':url}).count()\n",
    "        duplicate_notice(db_type, db, col, url, duplicate_count)\n",
    "        return True\n",
    "    \n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_url(source_code):\n",
    "    \"\"\"Takes a live url and it's BeautifulSoup object and pulls all the text from the body\n",
    "    Does not pull from inputs, scripts, and noscripts (to avoid JavaScript)\n",
    "    Returns the text as blog_text\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(source_code.text, \"lxml\", parse_only=only_body_tag)\n",
    "    blog_text = []\n",
    "    # still grabbing some javascript\n",
    "    for child in soup.body.children:\n",
    "        if child.name != None and child.name != 'script' and child.name != 'input' and child.name != 'noscript' and child.name != 'style' and child.name!= 'option':\n",
    "            blog_text.append(child.get_text(' ', strip=True))\n",
    "    space = ' '\n",
    "    blog_text = space.join(blog_text)\n",
    "    blog_text.encode('utf-8')\n",
    "    \n",
    "    return blog_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Needs to be Reworked! But for now we're turning it off to accept all pages\n",
    "# Runs inside of update_blog_entry\n",
    "def quality_check(url, blog_text):\n",
    "    \"\"\"Takes a url the scraped_text and passes it through a quality check.\n",
    "    \n",
    "    For this project that quality check is a Naive Bayes Classifier trained on categorized\n",
    "    blog posts from initial scraping.\n",
    "    \n",
    "    If the blog_text meets relevancy criteria, return True, else return False\n",
    "    \"\"\"\n",
    "    # Insert Naive Bayes classification here, if pass return True if fail return False\n",
    "    \"\"\"Example psuedo-code:\n",
    "    category = nb_classifier.predict(blog_text)\n",
    "    if category == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\"\"\"\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_blog_entry(url, blog_text, current_url):\n",
    "    \"\"\"If blog_text passes the quality check, update it's entry in mongo_db\n",
    "    Otherwise set open_seed to False and drop blog_text information\n",
    "    \n",
    "    For relevant web pages:\n",
    "    \n",
    "    hand_reviewed:\n",
    "    --- True if this entry has been hand reviewed\n",
    "    --- False if this entry has not been hand reviewed\n",
    "    \n",
    "    quality:\n",
    "    --- True if this web page passed quality check\n",
    "    --- False if this web page failed quality check\n",
    "    \"\"\"\n",
    "    if quality_check(url, blog_text):\n",
    "        if external_link(url, current_url):\n",
    "            col.insert_one(\n",
    "                {'url': url,\n",
    "                 'text': blog_text,\n",
    "                 'hand_reviewed': False,\n",
    "                 'quality': True,\n",
    "                 'open_seed':True,\n",
    "                 'inbound_link_count':1,\n",
    "                 'urls_pointed_here':[current_url]\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            col.insert_one(\n",
    "                {'url': url,\n",
    "                 'text': blog_text,\n",
    "                 'hand_reviewed': False,\n",
    "                 'quality': True,\n",
    "                 'open_seed':True,\n",
    "                 'inbound_link_count':0\n",
    "                }\n",
    "            )\n",
    "    else:\n",
    "        col.insert_one(\n",
    "            {'url':url,\n",
    "            'hand_reviewed':False,\n",
    "            'quality': False\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iterations = 225\n",
    "def run_crawler(max_iterations, location):\n",
    "    for _ in range(max_iterations):\n",
    "        current_url = get_seed()\n",
    "        source_code = make_request(current_url)\n",
    "        if source_code:\n",
    "            soup = BeautifulSoup(source_code.text, \"lxml\", parse_only=only_body_tag)\n",
    "            get_links(soup, current_url)\n",
    "        else:\n",
    "            pass\n",
    "    termination_notice(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_crawler(1, location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('593f0200f7e49c1cd22aea29'),\n",
       " 'hand_reviewed': False,\n",
       " 'inbound_link_count': 8,\n",
       " 'open_seed': True,\n",
       " 'quality': True,\n",
       " 'text': \" Business School Courses & Directory Business Schools Directory & Campus and Online mba Degrees Browse our MBA Info Library Home MBA Subjects MBA Schools Specialty MBAs MBA Library About Blog Explore MBA Subjects Accounting Business Administration Business Economics Communication Consulting Criminal Justice e-Business/e-Commerce Economics Education Engineering Entrepreneurship Executive / Mgt. Finance General Management Global Management Health Care / Administration Hospitality and Tourism Human Resources Industrial Management Information Systems / IT International Business Knowledge Management Leadership Marketing Media Nonprofit and Goverment Mgt. Operations Management Organizational Mgt. Project Management Real Estate Restaurant Enterprise Management Enterprise Management Service Management Sports Management Technology Management Telecommunications Discover our MBA Blog The 50 Best Tips for Small Businesses on Facebook As a small business, you may have heard what a great tool social media, and Facebook in particular, is for a business like... 50 Best Blogs for Following Asian Business It's impossible to be involved in international business today without paying attention to the economies of Asian nations... 10 Best Study Abroad Destinations for Business Majors Any student, no matter her or his major, should snap up any affordable opportunities to study abroad that present themselves.... Archives: May 2011 April 2011 March 2011 February 2011 January 2011 December 2010 December 2009 November 2009 April 2009 January 2009 November 2008 October 2008 Page Not Found Sorry, but you are looking for something that isn't here. Copyright © 2017 - BSchool.com Business Schools Directory | Privacy policy Homepage | MBA Info Library | International Side by Side Rankings | Top Business Schools\",\n",
       " 'url': 'http://www.bschool.com/blog/2011/40-fascinating-blogs-for-the-ultimate-statistics-geek/',\n",
       " 'urls_pointed_here': ['https://cooldata.wordpress.com/2013/07/30/getting-bitten-by-python/',\n",
       "  'https://cooldata.wordpress.com/2016/12/05/amazing-things-with-matching-strings/',\n",
       "  'https://cooldata.wordpress.com/2011/02/22/data-disasters-courtesy-of-mordac/',\n",
       "  'https://cooldata.wordpress.com/2015/01/07/new-finds-in-old-models/',\n",
       "  'https://cooldata.wordpress.com/2014/09/22/what-predictor-variables-should-you-avoid-depends-on-who-you-ask/',\n",
       "  'https://cooldata.wordpress.com/2013/06/27/time-management-for-data-analysts/',\n",
       "  'https://cooldata.wordpress.com/2015/08/26/exploring-associations-between-variables/',\n",
       "  'https://cooldata.wordpress.com/2012/04/24/data-i-want-to-play-with/']}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.backup.find()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x7fde2470df30>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update all entries in the collection\n",
    "# col.update_many({}, {'$set': {'inbound_link_count':0}})\n",
    "# col.update_many({},{'$set':{'open_seed': True}})\n",
    "\n",
    "# Should be 225 if no changes have been made yet:\n",
    "# col.find({'open_seed':True}).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove a field-value pair, useful for keeping storage free/size low\n",
    "# col.update_one({'url':url},{'$unset': {'urls_pointed_here':\"\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example Class from previous project for turning this into a class object\n",
    "# class weather_date:\n",
    "#     \"\"\"Class for scraping weather_data from www.wunderground.com\"\"\"\n",
    "#     def __init__(self, soup_obj, zipcode, date):\n",
    "#         self.date = date\n",
    "#         self.soup_obj = soup_obj\n",
    "#         self.zipcode = zipcode\n",
    "#         self.sunrise = self.get_sunrise(soup_obj)\n",
    "#         self.sunset = self.get_sunset(soup_obj)\n",
    "#         self.dew_point = self.get_dew_point(soup_obj)\n",
    "#         self.events = self.get_events(soup_obj)\n",
    "#         self.sea_level_pressure = self.get_sea_level_pressure(soup_obj)\n",
    "#         self.temp = self.get_temp(soup_obj)\n",
    "#         self.humidity = self.get_humidity(soup_obj)\n",
    "#         self.degree_days = self.get_degree_days(soup_obj)\n",
    "#         self.wind = self.get_wind(soup_obj)\n",
    "#         self.precipitation = self.get_precipitation(soup_obj)\n",
    "#         self.snow = self.get_snow(soup_obj)\n",
    "#         self.moon_phase = self.get_moon_phase(soup_obj)\n",
    "#         self.weather_dict = self.create_dict() \n",
    "    \n",
    "#     # Preliminary cleaning of data\n",
    "#     def clean(self, result):\n",
    "#         pattern = re.compile(r'\\\\n|\\\\xa0|\\\\xc2|\\\\t|b\\'|\\\\xb0|<\\w+>|</\\w+>|')\n",
    "#         return re.sub(pattern,'', str(result.encode('utf8')))\n",
    "    \n",
    "#     def get_sunrise(self, soup):\n",
    "#         sunrise = soup.find(text='Actual Time').find_next('td').text\n",
    "#         return self.clean(sunrise)\n",
    "    \n",
    "#     def get_sunset(self, soup):\n",
    "#         sunset = soup.find(text='Actual Time').find_next('td').find_next('td').text\n",
    "#         return self.clean(sunset)\n",
    "    \n",
    "#     def get_dew_point(self, soup):\n",
    "#         dew_point = soup.find(text='Dew Point').find_next('td').text\n",
    "#         return self.clean(dew_point)\n",
    "    \n",
    "#     def get_events(self, soup):\n",
    "#         events = soup.find(text='Events').find_next('td').text\n",
    "#         return self.clean(events)\n",
    "    \n",
    "#     def get_sea_level_pressure(self, soup):\n",
    "#         sea_level_pressure = soup.find(text='Sea Level Pressure').find_next(text='Sea Level Pressure').find_next('td').text\n",
    "#         return self.clean(sea_level_pressure)\n",
    "    \n",
    "#     def get_temp(self, soup):\n",
    "#         temp_regex = re.compile(' temperature', re.I)\n",
    "#         temp_entries = soup.find_all(text = temp_regex)\n",
    "#         temp_list = []\n",
    "#         for record in temp_entries:\n",
    "#             temp_list.append(self.clean(record.find_next('td').text))\n",
    "#         return temp_entries, temp_list\n",
    "    \n",
    "#     def get_humidity(self, soup):\n",
    "#         hum_regex = re.compile(' humidity', re.I)\n",
    "#         hum_entries = soup.find_all(text = hum_regex)\n",
    "#         temp_list = []\n",
    "#         for record in hum_entries:\n",
    "#             temp_list.append(record.find_next('td').text.encode('utf8'))\n",
    "#         return hum_entries, temp_list\n",
    "    \n",
    "#     def get_degree_days(self, soup):\n",
    "#         temp_list = []\n",
    "#         degree_days_regex = re.compile('ing degree days', re.I)\n",
    "#         degree_days = soup.find_all(text = degree_days_regex)\n",
    "#         for record in degree_days:\n",
    "#             temp_list.append(self.clean(record.find_next('td').text))\n",
    "#         return degree_days, temp_list\n",
    "    \n",
    "#     def get_wind(self, soup):\n",
    "#         wind_regex = re.compile(' speed', re.I)\n",
    "#         wind_entries = soup.find_all(text = wind_regex)\n",
    "#         wind_entries = wind_entries[:3]\n",
    "#         temp_list = []\n",
    "#         for record in wind_entries[:3]:\n",
    "#             temp_list.append(self.clean(record.find_next('td').text))\n",
    "#         return wind_entries, temp_list\n",
    "    \n",
    "#     def get_precipitation(self, soup):\n",
    "#         precipitation_regex = re.compile('precipitation', re.I)\n",
    "#         precipitation = soup.find_all(text=precipitation_regex)\n",
    "#         precipitation = precipitation[1:-1]\n",
    "#         temp_list = []\n",
    "#         for record in precipitation:\n",
    "#             temp_list.append(self.clean(record.find_next('td').text))\n",
    "#         return precipitation, temp_list\n",
    "    \n",
    "#     def get_snow(self, soup):\n",
    "#         snow_regex = re.compile('snow', re.I)\n",
    "#         snow = soup.find(class_='history-table-grey-header', text=snow_regex)\n",
    "#         snow_entries = ['Snow', 'Month to date snowfall', 'Since 1 July snowfall', 'Snow Depth']\n",
    "#         snow_data = []\n",
    "#         for x in range(5):\n",
    "#             try:\n",
    "#                 snow = snow.find_next(text=snow_regex)\n",
    "#                 snow_entries.append(self.clean(snow.encode))\n",
    "#                 snow_data.append(self.clean(snow.find_next('td').text))\n",
    "#             except AttributeError:\n",
    "#                 while len(snow_data) < len(snow_entries):\n",
    "#                     snow_data.append(np.nan)\n",
    "#             logging.debug('Error pulling snow for date %s' % date)\n",
    "#         return snow_entries, snow_data\n",
    "    \n",
    "#     def get_moon_phase(self, soup):\n",
    "#         moon_phase = soup.find(class_='phaseIcon').find_next('td').text\n",
    "#         return self.clean(moon_phase)\n",
    "\n",
    "#     def create_dict(self):\n",
    "#         temp_dict = {}\n",
    "#         temp_dict['Date'] = self.date\n",
    "#         temp_dict['Zipcode'] = self.zipcode\n",
    "#         temp_dict['Sunrise'] = self.sunrise\n",
    "#         temp_dict['Sunset'] = self.sunset\n",
    "#         temp_dict['Dew Point'] = self.dew_point\n",
    "#         temp_dict['Events'] = self.events\n",
    "#         temp_dict['Sea Level Pressure'] = self.sea_level_pressure\n",
    "#         for n in range(len(self.temp[0])):\n",
    "#             temp_dict[self.clean(self.temp[0][n])] = self.temp[1][n]\n",
    "#         for n in range(len(self.humidity[0])):\n",
    "#             temp_dict[self.clean(self.humidity[0][n])] = self.humidity[1][n]\n",
    "#         for n in range(len(self.degree_days[0])):\n",
    "#             temp_dict[self.clean(self.degree_days[0][n])] = self.degree_days[1][n]\n",
    "#         for n in range(len(self.wind[0])):\n",
    "#             temp_dict[self.clean(self.wind[0][n])] = self.wind[1][n]\n",
    "#         for n in range(len(self.precipitation[0])):\n",
    "#             temp_dict[self.clean(self.precipitation[0][n])] = self.precipitation[1][n]\n",
    "#         for n in range(len(self.snow[0])):\n",
    "#             temp_dict[self.clean(self.snow[0][n])] = self.snow[1][n]\n",
    "#         temp_dict['Moon Phase'] = self.moon_phase\n",
    "#         return temp_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
